# BERT-Question-Answer
BERT is a Bidirectional Encoder Representation from Transformers which consists of encoders pre-trained with the task to understand human language. BERT was initially pre-trained simultaneously based on predicting whether two sentences are continous and completing fill in the blank questions. The Question Answering BERT was a fine tuned version of BERT which is able to highlight answers to a question from a given passage. It does this by training a start and end vector along with the BERT representation of every word, and computing the dot product followed by a softmax activation to compute the probability of each word being the start or end word of the answer. 
